{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load some data. We'll use this in a bit. \n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/358223959 一篇文章搞懂logit, logistic和sigmoid的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "Linear regression allows us to make numerical predictions based on one or more inputs, either numerical or categorical. Logistic regression is the equivalent that allows us to make classfication predictions - predicting if something falls into group A or group B. \n",
    "\n",
    "Logistic regression is based on our old friend, the logit, of log ratio fame. We effectively are doing a regression to predict the likelihood of something happening, then categorizing it based on if it is more probable than some cutoff (e.g. 50%). For example, we can calculate the probability that a transaction is fraudulent, then if it is more likely than not to be fraud, we categorize it as such. For now, we'll look at predicting between two classes, but that's not a limit, we can categorize into many classes. \n",
    "\n",
    "Logistic regression (and other classification methods) are extremely common. Regression/prediction and classification are the two big pillars of predictive analytics that we will look at through next term. \n",
    "# 逻辑回归\n",
    "\n",
    "线性回归允许我们根据一个或多个输入（数值或分类）进行数值预测。逻辑回归等同于允许我们进行分类预测——预测某物是否属于 A 组或 B 组。\n",
    "\n",
    "Logistic 回归是基于我们的老朋友 logit，log ratio 的名声。我们实际上是在进行回归以预测某事发生的可能性，然后根据它是否比某个截止值（例如 50%）更有可能对其进行分类。例如，我们可以计算一笔交易是欺诈的概率，如果它是欺诈的可能性比不是欺诈的可能性大，我们就将其归类为欺诈。现在，我们将研究两个类别之间的预测，但这不是限制，我们可以分为许多类别。\n",
    "\n",
    "逻辑回归（和其他分类方法）非常普遍。回归/预测和分类是我们将在下学期研究的预测分析的两大支柱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Problem\n",
    "\n",
    "Dealing with classification is a little different than regression, because now we are not looking to predict a value, we are looking to predic a class - or phrased alternatively, we are looking to divide two (or more) sets of data.  \n",
    "\n",
    "If we plot a simple 2 varaible problem, just like we did in linear regression, we'll get something that looks like this:\n",
    "<ul>\n",
    "<li> Suppose that BMI is our X and Outcome (do you have diabetes?) is the Y. \n",
    "<li> Plot that on a scatter plot. \n",
    "<li> Our goal is to use X to predict Y, just as it was in linear regression. \n",
    "<li> However, there's not a very obvious way to use the X value only to do a linear regression that has any degree of accuracy.\n",
    "    <ul>\n",
    "    <li> Seriously, try to generate any line of best fit that doesn't have massive residuals. \n",
    "    </ul>\n",
    "</ul>\n",
    "## 分类问题\n",
    "\n",
    "处理分类与回归略有不同，因为现在我们不是要预测一个值，而是要预测一个类——或者换句话说，我们要划分两个（或更多）数据集。\n",
    "\n",
    "如果我们绘制一个简单的 2 变量问题，就像我们在线性回归中所做的那样，我们将得到如下所示的结果：\n",
    "<ul>\n",
    "<li> 假设 BMI 是我们的 X，结果（你有糖尿病吗？）是 Y。\n",
    "<li> 将其绘制在散点图上。\n",
    "<li> 我们的目标是使用 X 来预测 Y，就像在线性回归中一样。\n",
    "<li> 但是，没有一种非常明显的方法可以仅使用 X 值来执行具有任何准确度的线性回归。\n",
    "    <ul>\n",
    "    <li> 说真的，尝试生成任何没有大量残差的最佳拟合线。\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='BMI', ylabel='Outcome'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"BMI\", y=\"Outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So What Do We Do?\n",
    "We need something that can transform our simple linear fitting into something... else. \n",
    "\n",
    "＃＃＃＃ 那么我们该怎么办？\n",
    "我们需要一些可以将我们简单的线性拟合转换成其他东西的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the Logit?\n",
    "\n",
    "Recall from the probability stuff, we also calculated odds - or the ratio of something happening to it not happening. Odds are just an alternative expression of probability. E.g. if we are 30% likely to win a price, p = .3. We generally treat classification as splitting our data into 1 and 0, or true and not true, so the odds of the \"other option\" are allways just 1-odds. We can classfy into multiple categories (e.g. predict hair color as blonde, red, black, brown), the same ideas apply - we will examine this more next semester, for now it is all yes/no. If we translate that to odds:\n",
    "\n",
    "$ Odds = \\frac{.3}{(1-.3)} = \\frac{p}{(1-p)} = o $\n",
    "\n",
    "If we then take the log of that (remember, taking the log of the odds made things work), we end up with:\n",
    "\n",
    "$ log(odds) = log(\\frac{p}{(1-p)}) = log(\\frac{p}{not p}) = log(\\frac{prob(1)}{prob(0)}) $ (we can have two outcomes here - 1 or 0, so the probabilities are for either of those two happening)\n",
    "\n",
    "In logistic regression, we use the standard form:\n",
    "\n",
    "$ log(odds) = m*x + b $\n",
    "\n",
    "So, we are able to predict the log odds using the same linear regression format that we are used to. Thanks log odds! \n",
    "### 还记得 Logit 吗？\n",
    "\n",
    "回想一下概率，我们还计算了几率——或者某事发生与未发生的比率。赔率只是概率的另一种表达方式。例如。如果我们有 30% 的可能性赢得价格，则 p = .3。我们通常将分类视为将我们的数据分为 1 和 0，或者为真和不为真，因此“其他选项”的几率总是只有 1 的几率。我们可以分为多个类别（例如，预测头发颜色为金色、红色、黑色、棕色），同样的想法也适用——我们将在下学期对此进行更多研究，目前都是是/否。如果我们将其转化为赔率：\n",
    "\n",
    "$ 赔率 = \\frac{.3}{(1-.3)} = \\frac{p}{(1-p)} = o $\n",
    "\n",
    "如果我们然后对它进行记录（记住，对赔率进行记录使事情可行），我们最终得到：\n",
    "\n",
    "$ log(odds) = log(\\frac{p}{(1-p)}) = log(\\frac{p}{not p}) = log(\\frac{prob(1)}{prob(0) }) $（这里我们可以有两个结果——1 或 0，所以概率是这两个发生的任何一个）\n",
    "\n",
    "在逻辑回归中，我们使用标准形式：\n",
    "\n",
    "$ log(赔率) = m*x + b $\n",
    "\n",
    "因此，我们能够使用我们习惯的相同线性回归格式来预测对数几率。感谢日志赔率！\n",
    "\n",
    "#### Full Math\n",
    "\n",
    "![Logistic 1](images/logit_math_1.jpeg \"Logistic 1\")\n",
    "![Logistic 2](images/logit_math_2.jpeg \"Logistic 2\")\n",
    "\n",
    "## Enter the Sigmoid\n",
    "\n",
    "We end up here with a function called the sigmoid, which is what gives us our actual predictions. The sigmoid has the useful property that it \"jumps\" from 0 to 1 very quickly and never goes past those bounds. \n",
    "\n",
    "![Sigmoid](images/sigmoid.png \"Sigmoid\")\n",
    "\n",
    "What this all means is that we can calculate the probability - which is the output of the sigmoid - then just draw a cutoff to label our prediction as 1 or 0. \n",
    "\n",
    "![Classification](images/sig_prob.png \"Classification\")\n",
    "\n",
    "Since the sigmoid is so \"steep\", it tends to do a good job of separating - small changes in the input, while the values are near the transition part of the curve, yeild large changes in the prediction. \n",
    "\n",
    "<b>Note:</b> we don't need to derive this or manually calculate it, we mainly just want to understand the concept. \n",
    "#### 全数学\n",
    "\n",
    "![后勤 1](images/logit_math_1.jpeg \"后勤 1\")\n",
    "![后勤 2](images/logit_math_2.jpeg \"后勤 2\")\n",
    "\n",
    "## 输入 Sigmoid\n",
    "\n",
    "我们在这里结束了一个叫做 sigmoid 的函数，它给了我们实际的预测。 sigmoid 有一个有用的属性，它可以非常快速地从 0 跳到 1，并且永远不会超过这些界限。\n",
    "\n",
    "![Sigmoid](images/sigmoid.png \"Sigmoid\")\n",
    "\n",
    "这一切意味着我们可以计算概率——这是 sigmoid 的输出——然后画一个截止点来将我们的预测标记为 1 或 0。\n",
    "\n",
    "![分类](images/sig_prob.png \"分类\")\n",
    "\n",
    "由于 sigmoid 非常“陡峭”，它倾向于很好地分离 - 输入中的小变化，而值接近曲线的过渡部分，预测中产生大的变化。\n",
    "\n",
    "<b>注意：</b>我们不需要推导或手动计算它，我们主要只是想了解这个概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough Example\n",
    "\n",
    "Simple example... We want to predict Y, given some values of X. For this, we can say that the values are as follows:\n",
    "\n",
    "<ul>\n",
    "<li> Y = Passed high school. 1 = Yes.\n",
    "<li> X1 = Attended class. 1 = Yes.\n",
    "<li> X2 = Studied at home. 1 = Yes. \n",
    "</ul>\n",
    "\n",
    "Each set of values (a column) is one person, so we have two people who passed and two who did not. The details of the data don't matter much, we're looking at the mechanics here. We'll do a real one in a min. This middle part of the curve is sometimes called the Decision Boundary. \n",
    "## 演练示例\n",
    "\n",
    "简单的例子......我们想要预测 Y，给定一些 X 的值。为此，我们可以说这些值如下：\n",
    "\n",
    "<ul>\n",
    "<li> Y = 高中毕业。 1 = 是。\n",
    "<li> X1 = 已上课。 1 = 是。\n",
    "<li> X2 = 在家学习。 1 = 是。\n",
    "</ul>\n",
    "\n",
    "每组值（一列）代表一个人，所以我们有两个人通过了，两个人没有通过。数据的细节并不重要，我们在这里查看机制。我们会在一分钟内做一个真正的。曲线的中间部分有时称为决策边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = np.array([0, 1, 0, 1])\n",
    "x1 = np.array([0, 0, 0, 1])\n",
    "x2 = np.array([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a regression, using the logit formula:\n",
    "\n",
    "$\\log o = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $\n",
    "\n",
    "We don't know our coefficients though - the process for determining them isn't a direct calculation like linear regression. Here we need to try some, check our error, then improve. (This is a common thing in ML).\n",
    "\n",
    "For this, we are making an arbitrary guess. \n",
    "我们可以使用 logit 公式进行回归：\n",
    "\n",
    "$\\log o = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $\n",
    "\n",
    "虽然我们不知道我们的系数——确定它们的过程不是像线性回归那样的直接计算。这里我们需要尝试一些，检查我们的错误，然后改进。 （这在 ML 中很常见）。\n",
    "\n",
    "为此，我们进行了任意猜测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [-1.5, 2.8, 1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate it out, just like a linear regression. \n",
    "\n",
    "现在，我们可以计算出来，就像线性回归一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds_0=-1.5+2.8*0+1.1*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.4, -0.4,  2.4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_o = b[0] + b[1] * x1 + b[2] * x2\n",
    "log_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert log odds to odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22313016,  0.67032005,  0.67032005, 11.02317638])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = np.exp(log_o)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert odds to probabilities. These probabilities are the outputs of the sigmoid calculation, and we can use them to classify by just labeling things that are over the cutoff (usually .5) as 1s and the things that are under as 0. \n",
    "现在，将赔率转换为概率。这些概率是 sigmoid 计算的输出，我们可以使用它们进行分类，只需将超过截止值（通常为 .5）的事物标记为 1，将低于截止值的事物标记为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.40131234, 0.40131234, 0.9168273 ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = o / (o+1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Real Usage\n",
    "\n",
    "Those are all the predicted probabilities of each person passing high school. \n",
    "\n",
    "Now, a little weirdness. We started out this whole thing with some pretty random values for all the coefficients, so why would we trust these predictions? Well, right now, we wouldn't. What we need to do to make a model that is actually accurate is to check how well we did now, then make some improvements. \n",
    "\n",
    "To check how well we did now, we can calculate how close the probabilites are to the real values. E.g. Person #4 did really graduate, and our model predicted a ~92% chance of them graduating, that's good. Person #2 also graduated, but our model only predicted a ~40% chance of them graduating, that's bad. So our metric for evaluating is that we want our predictions to be as close as possible to the real values - or we want \"1\"s to have high percentages, and \"0\"s to have low percentages. The more sharpely we can discriminate between passes and fails, the more accurate the model. \n",
    "\n",
    "We can calculate this overall accuracy pretty simply - how likely are we to predict the correct answer? \n",
    "#### 实际使用\n",
    "\n",
    "这些都是每个人通过高中的预测概率。\n",
    "\n",
    "现在，有点奇怪。我们从所有系数的一些非常随机的值开始了整个事情，那么我们为什么要相信这些预测呢？好吧，现在，我们不会。要制作一个真正准确的模型，我们需要做的是检查我们现在做得有多好，然后进行一些改进。\n",
    "\n",
    "为了检查我们现在做得有多好，我们可以计算概率与真实值的接近程度。例如。第 4 个人确实毕业了，我们的模型预测他们毕业的几率约为 92%，这很好。人 #2 也毕业了，但我们的模型只预测了他们毕业的大约 40% 的机会，这很糟糕。所以我们的评估指标是我们希望我们的预测尽可能接近真实值——或者我们希望“1”的百分比高，“0”的百分比低。我们越能区分通过和失败，模型就越准确。\n",
    "\n",
    "我们可以非常简单地计算这个整体准确率——我们预测正确答案的可能性有多大？\n",
    "\n",
    "### Cost and Loss\n",
    "\n",
    "Some new concpets that are introduced here, and are important going forward, are the ideas of cost and loss. When doing these types of iteritive training processes our progress is tracked by our loss, or the amount of error. This amount is calculated by our loss function, or how that error is calculated. This is directly comparable to the MSE/RMSE process we looked at previously, we have some calculation to determine our overall accuracy. \n",
    "\n",
    "Each time we do an iteration, we get some amount of error, or loss. The best solution is where this loss is at it's lowest. This is the same idea as how the best linear regression model is best when the loss - the linear least squares distance - is at it's lowest. \n",
    "\n",
    "<b>Note:</b> the terms cost and loss are often used interchangably, and this is generally fine. Technically the loss function is for each specific example, and the cost is the overall summary. In practice, it is not that big of a deal to swap the terms - there won't really be many, if any, scenarios in which that will be confusing. \n",
    "### 成本和损失\n",
    "\n",
    "这里介绍的一些新概念对未来很重要，它们是成本和损失的概念。在进行这些类型的迭代训练过程时，我们的进度由我们的损失或错误量来跟踪。这个数量是通过我们的损失函数计算的，或者是如何计算误差的。这与我们之前查看的 MSE/RMSE 过程直接相当，我们有一些计算来确定我们的整体准确性。\n",
    "\n",
    "每次我们进行迭代时，我们都会得到一些错误或损失。最好的解决方案是这种损失最低的地方。这与当损失 - 线性最小二乘距离 - 最低时最佳线性回归模型的最佳状态相同。\n",
    "\n",
    "<b>注意：</b>成本和损失这两个术语经常可以互换使用，这通常没有问题。从技术上讲，损失函数是针对每个特定示例的，而成本是总体摘要。实际上，交换术语并没有什么大不了的——实际上不会有很多（如果有的话）混淆的场景。\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is the way that many algorithms try to minimize their loss. In short, this process is just:\n",
    "<ul>\n",
    "<li> Create a model. \n",
    "<li> Measure the loss. \n",
    "<li> Adjust the model's values (i.e. the slopes and intercepts here)\n",
    "<li> Check the loss again. \n",
    "<li> Repeat until you reach the lowest value for loss. (i.e. the most accurate model)\n",
    "</ul>\n",
    "\n",
    "![Gradient](images/grad_desc.png \"Gradient\")\n",
    "\n",
    "The algorithm \"knows which way to go\" in adjusting the weights between each trial via some calculus and matrix math that we will peek into when we look at neural networks. The algorithm can basically use partial derivitives to attribute error to the different values (the slopes), as well as if they are too high or too low. Each step moves these values a little, then we recheck. \n",
    "\n",
    "We will explore the details of gradient descent much more as we get into the machine learning stuff, for now understanding the general idea is good enough. This process is how most machine learning models \"learn\", and this is what is going on when they are processing for a long time. Each iteration moves the results (hopefully) to a point where the model has a little less error, and eventually we either \"find the bottom\" - which in logistic regression is the slopes and intercept values for the log-odds regression above, or we hit a limit of iterations. \n",
    "\n",
    "＃＃＃ 梯度下降\n",
    "\n",
    "梯度下降是许多算法试图最小化损失的方法。简而言之，这个过程就是：\n",
    "<ul>\n",
    "<li> 创建模型。\n",
    "<li> 测量损失。\n",
    "<li> 调整模型的值（即这里的斜率和截距）\n",
    "<li> 再次检查损失。\n",
    "<li> 重复直到达到最低损失值。 （即最准确的模型）\n",
    "</ul>\n",
    "\n",
    "![渐变](images/grad_desc.png \"渐变\")\n",
    "\n",
    "该算法通过一些微积分和矩阵数学来调整每次试验之间的权重，“知道该走哪条路”，我们在研究神经网络时会看到这些。该算法基本上可以使用偏导数将误差归因于不同的值（斜率），以及它们是否太高或太低。每一步都会稍微移动这些值，然后我们重新检查。\n",
    "\n",
    "当我们进入机器学习的东西时，我们将更多地探索梯度下降的细节，现在理解一般的想法就足够了。这个过程是大多数机器学习模型“学习”的方式，这也是它们在长时间处理时发生的情况。每次迭代都会（希望）将结果移动到模型误差较小的点，最终我们要么“找到底部”——在逻辑回归中就是上面对数-几率回归的斜率和截距值，或者我们达到迭代的极限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is our Loss?\n",
    "\n",
    "Our loss in the example above is a calculation the summarizes all of our individual errors. In the previous cell we calculated the probability of each person passing (being 1), our original Y data shows us the true probability of each person passing (either 0 or 1). Each prediction has an error of the distance between that true value and our expected probability. \n",
    "\n",
    "E.g. for the second item, this person passed, so the real value is 1. We predicted a ~40% likelihood of them passing, so our error there is ~60%. Person 4 passed, we predicted a ~92% chance of them passing, so our error is ~8%. \n",
    "##### 我们的损失是多少？\n",
    "\n",
    "我们在上面的例子中的损失是一个总结了我们所有个人错误的计算。在前面的单元格中，我们计算了每个人通过的概率（为 1），我们的原始 Y 数据向我们显示了每个人通过的真实概率（0 或 1）。每个预测都有一个真实值与我们预期概率之间的距离误差。\n",
    "\n",
    "例如。对于第二项，这个人通过了，所以实际值为 1。我们预测他们通过的可能性约为 40%，因此我们的误差约为 60%。第 4 个人通过了，我们预测他们通过的概率约为 92%，因此我们的误差为 8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.59868766, 0.40131234, 0.0831727 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#likes = np.where(y, p, 1-p)\n",
    "likes = np.where(y, 1-p, p)\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function\n",
    "\n",
    "These individual accuracies can be tallied up, we'll do a simple one with a common loss function called log-loss. There are a bunch of \"real\" loss functions that we can use, we'll explore them later on in machine learning. The most simple one is also based on the log of the odds, it is called Binary Cross-entropy, or Log-Loss. Don't worry about these details too much now, we will explore this later. \n",
    "\n",
    "![Log Loss](images/log_loss.png \"Log Loss\")\n",
    "\n",
    "The goal of the algorithm is to find the smallest value for this totalled loss, that's when we are most accurate overall. \n",
    "\n",
    "We can turn this loss total into an overall cost by just dividing by n. \n",
    "##### 损失函数\n",
    "\n",
    "可以计算这些单独的准确度，我们将使用一个称为对数损失的常见损失函数来做一个简单的。我们可以使用许多“真正的”损失函数，稍后我们将在机器学习中探索它们。最简单的也是基于几率的对数，叫做二元交叉熵，或者Log-Loss。现在不要太担心这些细节，我们稍后会探讨。\n",
    "\n",
    "![日志丢失](images/log_loss.png \"日志丢失\")\n",
    "\n",
    "该算法的目标是找到这个总损失的最小值，这是我们总体上最准确的时候。\n",
    "\n",
    "我们只需除以 n 即可将此总损失转化为总成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42856998373415184"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_sum = 0\n",
    "for i in range(len(p)):\n",
    "    if y[i] == 1:\n",
    "        loss_sum += -np.log(p[i])\n",
    "    elif y[i] == 0:\n",
    "        loss_sum += -np.log(1-p[i])\n",
    "cost = loss_sum/len(p)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall goal is to find the lowest possibility for this value. The lower this value, the closer to reality our model was predicting, the more accurate we can be. The process is to now to:\n",
    "<ul>\n",
    "<li> Take this amount of overall error, use it to make an adjustment to those starting values. (The ones we made up to start)\n",
    "    <ul>\n",
    "    <li> This step is something we'll look at in more depth with neural networks. In involves some partial derivitives which (kind of) allow us to work backwards and attribue parts of the errors to the original inputs. \n",
    "    </ul>\n",
    "<li> Calculate the new error with the different starting point. \n",
    "<li> Repeat - each stage should move us a little closer to the \"true\" answer. \n",
    "    <ul>\n",
    "    <li> In other words, we are repeating the process over and over until we've found the solution that minimizes our overal cost/loss (the amount of error). \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This process is called Gradient Descent and is something we will care about more in ML class. Basically we define something called a loss function, which measures how much error we have. We then repeat a bunch of trials with different coef values, and measure the loss each time. We keep repeating until we've found the lowest amount of loss - or the smallest amount of error. The math can be complex, but the idea is pretty simple. If we manually changed the array of b values, ran the model, collected the LIKE value, and finally selected the combination with the best LIKE, that'd be a crude version of the same thing. This idea is common later on. Here, sklearn or statsmodels do it for us. \n",
    "我们的总体目标是找到该值的最低可能性。这个值越低，我们的模型预测的越接近现实，我们就越准确。现在的过程是：\n",
    "<ul>\n",
    "<li> 取这个总误差量，用它来调整那些起始值。 （我们编造的开始）\n",
    "    <ul>\n",
    "    <li> 这一步我们将更深入地研究神经网络。 In 涉及一些偏导数，它们（某种程度上）允许我们向后工作并将部分错误归因于原始输入。\n",
    "    </ul>\n",
    "<li> 计算不同起点的新误差。\n",
    "<li> 重复 - 每个阶段都应该让我们更接近“真实”答案。\n",
    "    <ul>\n",
    "    <li> 换句话说，我们不断地重复这个过程，直到找到能够最大限度地降低总体成本/损失（错误量）的解决方案。\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "这个过程称为梯度下降，是我们在 ML 课程中会更关心的内容。基本上我们定义了一个叫做损失函数的东西，它衡量我们有多少错误。然后我们用不同的 coef 值重复一系列试验，并测量每次的损失。我们不断重复，直到我们找到最低的损失量 - 或者最小的错误量。数学可能很复杂，但想法很简单。如果我们手动更改 b 值数组，运行模型，收集 LIKE 值，最后选择具有最佳 LIKE 的组合，那将是同一事物的粗略版本。这种想法后来很普遍。在这里，sklearn 或 statsmodels 为我们做这件事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid and the Regression\n",
    "\n",
    "The sigmoid function is a function that takes in inputs (X values) and squishes all the outputs (Y values) between 0 and 1. The sigmoid is also the inverse of the logit function. The function is:\n",
    "\n",
    "$ g(x) = \\frac{1}{(1+e^-x)} = logit^-1 $\n",
    "\n",
    "A graph of what it ends up looking like is below. (Ignore the red line for now). The important part is now we have a way to connect the probabilities to our sigmoid function. \n",
    "### Sigmoid 和回归\n",
    "\n",
    "sigmoid 函数是一个接受输入（X 值）并将所有输出（Y 值）压缩在 0 和 1 之间的函数。sigmoid 也是 logit 函数的反函数。功能是：\n",
    "\n",
    "$ g(x) = \\frac{1}{(1+e^-x)} = logit^-1 $\n",
    "\n",
    "下面是它最终看起来的样子的图表。 （暂时忽略红线）。重要的是现在我们有办法将概率连接到我们的 sigmoid 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating vectors X and Y\n",
    "x = np.linspace(-5, 5, 10000)\n",
    "y = sigmoid(x)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "# Show the plot\n",
    "plt.axhline(.5, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression is going to use this sigmoid functions to generate a prediction between 0 and 1. Can can plug the linear regression equation into the sigmoid function, then our new hypothesis becomes:\n",
    "\n",
    "$ y = \\frac{1}{(1+e^(m*x+b))} $\n",
    "\n",
    "Where y = probability. \n",
    "\n",
    "Note - the mathmatical derivations aren't really super-duper critical. If it is confusing, just ingore it. There's a full derivation and example here: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\n",
    "我们的逻辑回归就是要用这个sigmoid函数来产生0到1之间的预测。可以把线性回归方程代入sigmoid函数，那么我们的新假设就变成了：\n",
    "\n",
    "$ y = \\frac{1}{(1+e^(m*x+b))} $\n",
    "\n",
    "其中 y = 概率。\n",
    "\n",
    "注意 - 数学推导并不是真正的超级关键。如果它令人困惑，请忽略它。这里有一个完整的推导和例子：https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Real Example - SciKitLearn</h2>\n",
    "\n",
    "For our first try we can use the diabetes example we've used a bit before, though we've always sidesteped the true target. The outcome value is whether or not someone is diabetic, and all the other variables that are risk factors that we can use to predict if someone will become diabetic. Our aim is to predict, yes or no, will someone develop diabetes based on those risk factors. \n",
    "\n",
    "First - one variable. We'll use BMI.\n",
    "<h2>真实示例 - SciKitLearn</h2>\n",
    "\n",
    "对于我们的第一次尝试，我们可以使用我们之前使用过的糖尿病示例，尽管我们总是回避真正的目标。结果值是某人是否患有糖尿病，以及所有其他变量，这些变量是我们可以用来预测某人是否会患上糖尿病的风险因素。我们的目标是预测，是或否，是否有人会根据这些风险因素患上糖尿病。\n",
    "\n",
    "第一 - 一个变量。我们将使用 BMI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1) (768, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"Outcome\"]).reshape(-1,1)\n",
    "x = np.array(df[\"BMI\"]).reshape(-1,1)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ravel()</b> - sometimes you may get a message that says something like \"we want the y data in the shape (samples,)\". This is obviously a contradiction to what we said to always shape the y as (samples, 1). The easiest way to deal with this is to use the .ravel() function as shown below. The cause is the expectation of data format for whatever you're using, which can vary. If we always make the y array (samples, 1) and then use ravel when needed, that allows us to be consistent and not worry about it much. I'd suggest keeping with this for simplicity. \n",
    "<b>ravel()</b> - 有时您可能会收到一条消息，内容类似于“我们想要形状为 (samples,) 的 y 数据”。这显然与我们所说的始终将 y 塑造为 (samples, 1) 相矛盾。处理这个问题的最简单方法是使用 .ravel() 函数，如下所示。原因是对您使用的任何数据格式的期望，这可能会有所不同。如果我们总是创建 y 数组 (samples, 1) 然后在需要的时候使用 ravel，那么我们就可以保持一致而不必担心太多。为了简单起见，我建议保持这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125984251968503\n"
     ]
    }
   ],
   "source": [
    "md1 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md1Pred = md1.predict(X_test)\n",
    "\n",
    "score = md1.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66% accuracy. Not bad. We can try with more Xs though...\n",
    "\n",
    "#### Multiple Logistic Regression\n",
    "\n",
    "66% 的准确率。不错。不过我们可以尝试使用更多的 X...\n",
    "\n",
    "#### 多元逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all the X values. \n",
    "# I can use the y from above still \n",
    "#获取所有的X值。\n",
    "# 我仍然可以使用上面的 y\n",
    "\n",
    "df2 = df.drop(columns={\"Outcome\"})\n",
    "x2 = np.array(df2)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755905511811023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Tools\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#I'm reusing some varaible names to make my life easier with copy/paste. \n",
    "#Make sure you run things in order if you do this. \n",
    "#I 正在重复使用一些可变名称，以便通过复制/粘贴让我的生活更轻松。\n",
    "#如果执行此操作，请确保按顺序运行。\n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y, test_size=0.33)\n",
    "\n",
    "md2 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this the first time, I didn't get an answer, I instead got something along the lines of \"failed to converge\". This means that the gradient descent process didn't finish, and the algorithm didn't settle on an answer. We will explore this more in the machine learning stuff, for now we can just tell it to set a higher cap on how long it can run. \n",
    "当我第一次运行它时，我没有得到答案，而是得到了类似“无法收敛”的信息。这意味着梯度下降过程没有完成，算法也没有确定答案。我们将在机器学习方面对此进行更多探索，现在我们可以告诉它为它可以运行多长时间设置一个更高的上限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637795275590551\n"
     ]
    }
   ],
   "source": [
    "md2 = LogisticRegression(max_iter=1000).fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results \n",
    "\n",
    "We can demonstrate some results... We'll look into result details more later. \n",
    "### 分类结果\n",
    "\n",
    "我们可以展示一些结果……我们稍后会详细研究结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix \n",
    "preds = md2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83       157\n",
      "           1       0.79      0.52      0.62        97\n",
      "\n",
      "    accuracy                           0.76       254\n",
      "   macro avg       0.77      0.72      0.73       254\n",
      "weighted avg       0.77      0.76      0.75       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144  13]\n",
      " [ 47  50]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.37795275590551\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Work Through Titanic</h1>\n",
    "\n",
    "Predict who lives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "dfe = pd.read_csv(\"data/train.csv\")\n",
    "dfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe[\"no_cabin\"] = dfe[\"Cabin\"].isnull()\n",
    "dfe[\"family\"] = dfe[\"SibSp\"] + dfe[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped age due to missing values. Think about if there's anything else we may want to do to it instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>no_cabin</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex     Fare Embarked  no_cabin  family\n",
       "0         0       3    male   7.2500        S      True       1\n",
       "1         1       1  female  71.2833        C     False       1\n",
       "2         1       3  female   7.9250        S      True       0\n",
       "3         1       1  female  53.1000        S     False       1\n",
       "4         0       3    male   8.0500        S      True       0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfe2 = dfe.drop(columns={\"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\", \"PassengerId\", \"Age\"})\n",
    "dfe2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 7), (891, 1))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redo the dummy variables. \n",
    "dfe2_dumb = pd.get_dummies(dfe2, drop_first=True)\n",
    "\n",
    "ye = np.array(dfe2_dumb[\"Survived\"]).reshape(-1,1)\n",
    "xe = dfe2_dumb.drop(columns={\"Survived\"})\n",
    "xe.shape, ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7892376681614349"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainT, X_testT, y_trainT, y_testT = train_test_split(xe, ye)\n",
    "\n",
    "titan = LogisticRegression(max_iter=1000).fit(X_trainT,y_trainT.ravel())\n",
    "titan_preds = titan.predict(X_testT)\n",
    "scoreT = titan.score(X_testT, y_testT)\n",
    "scoreT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d5428c2c64b61772a58a6d322ada93ef792a54746f377a0b6cb473db1f3ca36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
